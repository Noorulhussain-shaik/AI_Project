{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfbbdffc",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "69027cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, accuracy_score\n",
    "from aif360.algorithms.preprocessing import Reweighing\n",
    "from aif360.datasets import StandardDataset\n",
    "from aif360.metrics import ClassificationMetric\n",
    "import cvxpy as cp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c136ced",
   "metadata": {},
   "source": [
    "## Loading and Transforming the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "7c86b6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv('D:/AI_Project/credit_score.csv')\n",
    "\n",
    "# Transform the target variable into a binary variable\n",
    "target_variable = 'CREDIT_SCORE'\n",
    "data['target'] = (data[target_variable] >= 700).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "719fd5d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in target variable before preprocessing: [0 1]\n",
      "Unique values in protected attribute: [1 0]\n"
     ]
    }
   ],
   "source": [
    "# Verify the unique values of the target variable\n",
    "print(\"Unique values in target variable before preprocessing:\", data['target'].unique())\n",
    "\n",
    "# Assuming 'DEFAULT' is the protected attribute\n",
    "protected_attribute = 'DEFAULT'\n",
    "print(\"Unique values in protected attribute:\", data[protected_attribute].unique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c90998",
   "metadata": {},
   "source": [
    "## Preprocessing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "9fdc08af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "def preprocess_data(df, target):\n",
    "    # Handling missing values\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].median())\n",
    "    \n",
    "    # Encoding categorical variables\n",
    "    categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "    df = pd.get_dummies(df, columns=categorical_cols, drop_first=True)\n",
    "    \n",
    "    # Standardizing numerical features (excluding the target column)\n",
    "    scaler = StandardScaler()\n",
    "    numerical_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    numerical_cols = numerical_cols.drop(target)  # Exclude the target column\n",
    "    df[numerical_cols] = scaler.fit_transform(df[numerical_cols])\n",
    "    \n",
    "    return df\n",
    "\n",
    "data = preprocess_data(data, 'target')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "54008528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in target variable after preprocessing: [0 1]\n"
     ]
    }
   ],
   "source": [
    "# Verify the unique values of the target variable after preprocessing\n",
    "print(\"Unique values in target variable after preprocessing:\", data['target'].unique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7d409e",
   "metadata": {},
   "source": [
    "## Spliting the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "89e74bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in y_train: [0 1]\n",
      "Unique values in y_test: [0 1]\n"
     ]
    }
   ],
   "source": [
    "# Splitting the data\n",
    "X = data.drop(['target', target_variable], axis=1)\n",
    "y = data['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Verify the splits\n",
    "print(\"Unique values in y_train:\", y_train.unique())\n",
    "print(\"Unique values in y_test:\", y_test.unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "5f82ebb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure y_train and y_test have binary values\n",
    "if not set(y_train.unique()).issubset({0, 1}):\n",
    "    raise ValueError(\"The target variable y_train contains values other than 0 and 1.\")\n",
    "if not set(y_test.unique()).issubset({0, 1}):\n",
    "    raise ValueError(\"The target variable y_test contains values other than 0 and 1.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "a2f3f0e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score: 0.5\n",
      "Confusion Matrix:\n",
      " [[199   0]\n",
      " [  1   0]]\n"
     ]
    }
   ],
   "source": [
    "# Train a logistic regression model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = model.predict(X_test)\n",
    "roc_score = roc_auc_score(y_test, y_pred)\n",
    "confusion = confusion_matrix(y_test, y_pred)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"ROC AUC Score:\", roc_score)\n",
    "print(\"Confusion Matrix:\\n\", confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "4698002c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disparate Impact before mitigation: 0.432\n",
      "Equal Opportunity Difference before mitigation: 0.278\n"
     ]
    }
   ],
   "source": [
    "# Creating AIF360 Datasets for Fairness Metrics Calculation\n",
    "train_data = pd.concat([X_train, y_train], axis=1)\n",
    "test_data = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "\n",
    "privileged_groups = [{'DEFAULT': 1}]\n",
    "unprivileged_groups = [{'DEFAULT': 0}]\n",
    "\n",
    "dataset_train = BinaryLabelDataset(df=train_data, label_names=['target'], protected_attribute_names=['DEFAULT'])\n",
    "dataset_test = BinaryLabelDataset(df=test_data, label_names=['target'], protected_attribute_names=['DEFAULT'])\n",
    "\n",
    "# Calculate fairness metrics before mitigation\n",
    "metric = ClassificationMetric(dataset_train, dataset_test,\n",
    "                              unprivileged_groups=unprivileged_groups,\n",
    "                              privileged_groups=privileged_groups)\n",
    "\n",
    "di_before = metric.disparate_impact()\n",
    "eod_before = metric.equal_opportunity_difference()\n",
    "print(\"Disparate Impact before mitigation:\", di_before)\n",
    "print(\"Equal Opportunity Difference before mitigation:\", eod_before)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8873c2",
   "metadata": {},
   "source": [
    "## Creating AIF360 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "88417d2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:[1.0] listed but not observed for feature DEFAULT\n"
     ]
    }
   ],
   "source": [
    "# Create aif360 dataset\n",
    "privileged_groups = [{protected_attribute: 1}]\n",
    "unprivileged_groups = [{protected_attribute: 0}]\n",
    "dataset = StandardDataset(df=pd.concat([X_train, y_train], axis=1), \n",
    "                          label_name='target', \n",
    "                          favorable_classes=[1], \n",
    "                          protected_attribute_names=[protected_attribute], \n",
    "                          privileged_classes=[[1]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b549fb",
   "metadata": {},
   "source": [
    "## Applying Reweighing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "abcf9e39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Laptop\\anaconda3\\lib\\site-packages\\aif360\\algorithms\\preprocessing\\reweighing.py:66: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  self.w_p_fav = n_fav*n_p / (n*n_p_fav)\n",
      "C:\\Users\\Laptop\\anaconda3\\lib\\site-packages\\aif360\\algorithms\\preprocessing\\reweighing.py:67: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  self.w_p_unfav = n_unfav*n_p / (n*n_p_unfav)\n",
      "C:\\Users\\Laptop\\anaconda3\\lib\\site-packages\\aif360\\algorithms\\preprocessing\\reweighing.py:68: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  self.w_up_fav = n_fav*n_up / (n*n_up_fav)\n",
      "C:\\Users\\Laptop\\anaconda3\\lib\\site-packages\\aif360\\algorithms\\preprocessing\\reweighing.py:69: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  self.w_up_unfav = n_unfav*n_up / (n*n_up_unfav)\n"
     ]
    }
   ],
   "source": [
    "# Apply Reweighing for bias mitigation\n",
    "RW = Reweighing(unprivileged_groups=unprivileged_groups, privileged_groups=privileged_groups)\n",
    "RW.fit(dataset)\n",
    "dataset_transf = RW.transform(dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57efdc2",
   "metadata": {},
   "source": [
    "## Model Development"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc198c16",
   "metadata": {},
   "source": [
    "### Model development with Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "4f4144fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Logistic Regression ROC AUC: 0.5\n"
     ]
    }
   ],
   "source": [
    "# Model Development: Logistic Regression\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "roc_auc = roc_auc_score(y_test, y_pred)\n",
    "print(f\"Baseline Logistic Regression ROC AUC: {roc_auc}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd212abe",
   "metadata": {},
   "source": [
    "### Model development: DRO Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "3e0cdf92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DRO Logistic Regression ROC AUC: 0.9698492462311558\n"
     ]
    }
   ],
   "source": [
    "# Model Development: DRO Logistic Regression\n",
    "def DRO_Logistic_Regression(X, y, rho=0.1):\n",
    "    n, d = X.shape\n",
    "    w = cp.Variable(d)\n",
    "    t = cp.Variable()\n",
    "    \n",
    "    objective = cp.Minimize(t + rho * cp.sum(cp.logistic(-cp.multiply(y, X @ w))))\n",
    "    constraints = [y[i] * (X[i] @ w) >= 1 - t for i in range(n)]\n",
    "    \n",
    "    prob = cp.Problem(objective, constraints)\n",
    "    prob.solve()\n",
    "    \n",
    "    return w.value\n",
    "\n",
    "# Apply DRO Logistic Regression\n",
    "w_dro = DRO_Logistic_Regression(X_train.values, y_train.values)\n",
    "y_pred_dro = np.dot(X_test, w_dro)\n",
    "roc_auc_dro = roc_auc_score(y_test, y_pred_dro)\n",
    "print(f\"DRO Logistic Regression ROC AUC: {roc_auc_dro}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f81836",
   "metadata": {},
   "source": [
    "## Evaluating Fairness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "e732ab11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disparate Impact: 1.3607146789285698\n",
      "Equal Opportunity Difference: 0.02720543834287903\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Fairness\n",
    "metric = ClassificationMetric(dataset, dataset_transf, unprivileged_groups=unprivileged_groups, privileged_groups=privileged_groups)\n",
    "print(f\"Disparate Impact: {metric.disparate_impact()}\")\n",
    "print(f\"Equal Opportunity Difference: {metric.equal_opportunity_difference()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f112858",
   "metadata": {},
   "source": [
    "## Continous Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "998e56ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monitoring ROC AUC: 0.5\n",
      "Confusion Matrix: \n",
      "[[199   0]\n",
      " [  1   0]]\n"
     ]
    }
   ],
   "source": [
    "# Continuous Monitoring\n",
    "def monitor_model(model, X_test, y_test):\n",
    "    # Implement continuous monitoring to check for bias and performance\n",
    "    y_pred = model.predict(X_test)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    return roc_auc, cm\n",
    "\n",
    "roc_auc, cm = monitor_model(model, X_test, y_test)\n",
    "print(f\"Monitoring ROC AUC: {roc_auc}\")\n",
    "print(f\"Confusion Matrix: \\n{cm}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54227bf6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
